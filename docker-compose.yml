services:
  ollama:
    image: ollama/ollama:0.12.3
    container_name: ollama
    restart: unless-stopped

    # ✅ Простий і сумісний спосіб дати контейнеру доступ до GPU
    gpus: all   # еквівалент docker run --gpus all

    # альтернатива/дублер для старіших середовищ Compose (можеш залишити обидва)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
              
    environment:
      # Примусово вибираємо CUDA-бібліотеку Ollama (динамічна "cuda_v11")
      - OLLAMA_LLM_LIBRARY=cuda    # див. список доступних бібліотек
      # Pascal — без FlashAttention
      - OLLAMA_FLASH_ATTENTION=0
      # Не обмежуємо VRAM штучно (видаляємо ОLLAMA_MAX_VRAM)
      # - OLLAMA_MAX_VRAM=0   # (НЕ ставимо; за замовчуванням необмежено)
      - OLLAMA_KEEP_ALIVE=10m
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_QUEUE=512
      - OLLAMA_KV_CACHE_TYPE=f16
      # Коректні змінні для runtime NVIDIA
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_KEEP_ALIVE=10m

      # Для WSL2 корисно, щоб libcuda з /usr/lib/wsl/lib точно потрапила в пошук
      - LD_LIBRARY_PATH=/usr/lib/wsl/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib/x86_64-linux-gnu

    ports:
      - "11434:11434"
      
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 10s

    volumes:
      - ollama:/root/.ollama

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_API_BASE_URL=http://ollama:11434
      - RAG_EMBEDDING_ENGINE=ollama
      - RAG_EMBEDDING_MODEL=nomic-embed-text
      - ENABLE_RAG_WEB_SEARCH=true
      - RAG_EMBEDDING_MODEL_AUTO_UPDATE=false
      - WHISPER_MODEL=base
      - WHISPER_MODEL_AUTO_UPDATE=false
      - UVICORN_LOOP=asyncio
    ports:
      - "3000:8080"
    security_opt:
      - seccomp=unconfined
      - apparmor=unconfined
    volumes:
      - openwebui:/app/backend/data

  proxmox-controller:
    build:
      context: .
      args:
        INSTALL_GPU_EXTRAS: ${INSTALL_GPU_EXTRAS:-false}
    container_name: proxmox-controller
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - PROXMOX_HOST=${PROXMOX_HOST}
      - PROXMOX_PORT=${PROXMOX_PORT:-8006}
      - PROXMOX_USER=${PROXMOX_USER}
      - PROXMOX_TOKEN_NAME=${PROXMOX_TOKEN_NAME}
      - PROXMOX_TOKEN_VALUE=${PROXMOX_TOKEN_VALUE}
      - PROXMOX_VERIFY_SSL=${PROXMOX_VERIFY_SSL:-False}
      - CORS_ALLOW_ORIGINS=${CORS_ALLOW_ORIGINS:-*}
      - PVE_SSH_HOST=${PVE_SSH_HOST}
      - PVE_SSH_USER=${PVE_SSH_USER:-root}
      - PVE_SSH_KEY_PATH=/keys/pve_id_rsa
      - BLISS_ADB_ADDRESS=${BLISS_ADB_ADDRESS:-}
      - BLISS_ADB_HOST=${BLISS_ADB_HOST:-}
      - BLISS_ADB_PORT=${BLISS_ADB_PORT:-5555}
      - BLISS_ADB_SERIAL=${BLISS_ADB_SERIAL:-}
      - ADB_BINARY=${ADB_BINARY:-}
      - BLISS_OPENAPI_PATH=${BLISS_OPENAPI_PATH:-}
      - UVICORN_LOOP=asyncio
    command: ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000", "--loop", "asyncio"]
    ports:
      - "8000:8000"
    security_opt:
      - seccomp=unconfined
      - apparmor=unconfined
    volumes:
      - ./keys:/keys:ro
      - ./openapi.json:/app/openapi.json:ro
      - ./openapi_bliss.json:/app/openapi_bliss.json:ro
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 10s

volumes:
  ollama:
  openwebui:
