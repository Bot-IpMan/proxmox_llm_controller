version: "3.9"

# docker-compose.yml (fixed for кращу утилізацію GPU на GTX 1050 Ti)
# Головні правки:
# 1) Паралельність виставляється на сервері Ollama (OLLAMA_NUM_PARALLEL), а не в UI.
# 2) Усі batch/ubatch/num_ctx/num_thread — задаються в Modelfile або в API options (а НЕ через OLLAMA_* env).
# 3) Прибрано зайві змінні середовища, які Ollama не читає на рівні сервера.

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      # Дозволяємо CORS з будь-якого оріджина (для Open WebUI тощо)
      - OLLAMA_ORIGINS=*
      # Тримати модель у пам’яті довше для перевикористання
      - OLLAMA_KEEP_ALIVE=10m
      # Ключове: скільки паралельних запитів ОДНА модель може обробляти
      - OLLAMA_NUM_PARALLEL=8
      # Завантажуємо тільки одну модель одночасно — максимум ресурсів на неї
      - OLLAMA_MAX_LOADED_MODELS=1
      # Довша черга запитів, аби не відбивало 503 під навантаженням
      - OLLAMA_MAX_QUEUE=1024
      # Оберіть GPU (0 — перша карта)
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # Часовий пояс
      - TZ=Europe/Kyiv
    # Якщо ти використовуєш скрипт вибору GPU — залишаємо його
    entrypoint: ["/usr/local/bin/ollama-select-gpu.sh"]
    command: ["serve"]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 10s
    volumes:
      - ollama:/root/.ollama
      - ./scripts/ollama-select-gpu.sh:/usr/local/bin/ollama-select-gpu.sh:ro
    # GPU для контейнера (Compose офіційно)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1           # або 'all'
              capabilities: [gpu]

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    platform: linux/amd64
    depends_on:
      ollama:
        condition: service_healthy
    security_opt:
      - seccomp=unconfined
      - apparmor=unconfined
    environment:
      # Під’єднання до Ollama
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_API_BASE_URL=http://ollama:11434
      # RAG/Whisper — лишив як у тебе
      - RAG_EMBEDDING_ENGINE=ollama
      - RAG_EMBEDDING_MODEL=nomic-embed-text
      - ENABLE_RAG_WEB_SEARCH=true
      - RAG_EMBEDDING_MODEL_AUTO_UPDATE=false
      - WHISPER_MODEL=base
      - WHISPER_MODEL_AUTO_UPDATE=false
      - UVICORN_LOOP=asyncio
      - TZ=Europe/Kyiv
      # ВАЖЛИВО: НЕ ставимо тут OLLAMA_NUM_PARALLEL — це змінна сервера Ollama.
    ports:
      - "3000:8080"
    volumes:
      - openwebui:/app/backend/data

  proxmox-controller:
    build:
      context: .
      args:
        INSTALL_GPU_EXTRAS: ${INSTALL_GPU_EXTRAS:-false}
    container_name: proxmox-controller
    restart: unless-stopped
    depends_on: [ollama]
    security_opt:
      - seccomp=unconfined
      - apparmor=unconfined
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - PROXMOX_HOST=${PROXMOX_HOST}
      - PROXMOX_PORT=${PROXMOX_PORT:-8006}
      - PROXMOX_USER=${PROXMOX_USER}
      - PROXMOX_TOKEN_NAME=${PROXMOX_TOKEN_NAME}
      - PROXMOX_TOKEN_VALUE=${PROXMOX_TOKEN_VALUE}
      - PROXMOX_VERIFY_SSL=${PROXMOX_VERIFY_SSL:-False}
      - CORS_ALLOW_ORIGINS=${CORS_ALLOW_ORIGINS:-*}
      - PVE_SSH_HOST=${PVE_SSH_HOST}
      - PVE_SSH_USER=${PVE_SSH_USER:-root}
      - PVE_SSH_KEY_PATH=/keys/pve_id_rsa
      - BLISS_ADB_ADDRESS=${BLISS_ADB_ADDRESS:-}
      - BLISS_ADB_HOST=${BLISS_ADB_HOST:-}
      - BLISS_ADB_PORT=${BLISS_ADB_PORT:-5555}
      - BLISS_ADB_SERIAL=${BLISS_ADB_SERIAL:-}
      - ADB_BINARY=${ADB_BINARY:-}
      - BLISS_OPENAPI_PATH=${BLISS_OPENAPI_PATH:-}
      - UVICORN_LOOP=asyncio
      - TZ=Europe/Kyiv
    volumes:
      - ./keys:/keys:ro
      - ./openapi.json:/app/openapi.json:ro
      - ./openapi_bliss.json:/app/openapi_bliss.json:ro
    command: ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000", "--loop", "asyncio"]
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 10s

volumes:
  ollama:
  openwebui:

# === Підказки (коментарі) ===
# • Параметри типу num_batch / num_ctx / num_thread / num_gpu треба задавати
#   в Modelfile через 'PARAMETER ...' або в 'options' під час API-виклику.
#   Приклади Modelfile:
#
#   FROM llama3.1:8b-instruct
#   PARAMETER num_ctx 4096
#   PARAMETER num_batch 512
#   PARAMETER num_thread 6
#   PARAMETER num_gpu 1
#
# • Для тесту завантаження GPU зроби кілька паралельних викликів до Ollama API.
